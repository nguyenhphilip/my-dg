<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>statistics on</title><link>https://nguyenhphilip.github.io/tags/statistics/</link><description>Recent content in statistics on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://nguyenhphilip.github.io/tags/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Bayes Factor</title><link>https://nguyenhphilip.github.io/notes/statistics/Bayes-factor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Bayes-factor/</guid><description>A video introduction</description></item><item><title>bias-variance tradeoff</title><link>https://nguyenhphilip.github.io/notes/statistics/bias-variance-tradeoff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/bias-variance-tradeoff/</guid><description>variance and bias refer to under and overfitting on the data.
Wikipedia on the bias-variance tradeoff: bias is error from erroneous assumptions in the ==model/learning algorithm== [e.g. using linear models when the data is non-linear -&amp;gt; high bias, more generalizable; not using enough predictors or predictors that may be important to the outcome -&amp;gt; high bias]. High bias causes the model to miss the relevant relations between features and target outputs, which leads to ==underfitting==.</description></item><item><title>binomial distribution</title><link>https://nguyenhphilip.github.io/notes/statistics/binomial-distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/binomial-distribution/</guid><description>The binomial distribution is the generalized version of the Bernoulli distribution where instead of calculating the probability of success of a single event, we want the probability of $k$ successes in $n$ trials (in the Bernoulli $n=1$).
The probability of $k$ successful outcomes over $n$ trials is given by : $$P_k = {n \choose k}{p^k}{(1-p)^{n-k}}$$
The binomial distribution is the most consistent distribution - it maximizes entropy - given these constraints:</description></item><item><title>bootstrap method</title><link>https://nguyenhphilip.github.io/notes/statistics/bootstrap-method/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/bootstrap-method/</guid><description>repeatedly resampling the data with replacement, and then using the distribution of the statistic computed on those samples as a surrogate for the sampling distribution of the statistic</description></item><item><title>causal inference</title><link>https://nguyenhphilip.github.io/notes/statistics/causal-inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/causal-inference/</guid><description>Introduction Something counterintuitive about correlation Causation does not necessarily imply correlation .
Mixtape example: the sailor who is moving the rudder in response to the wind. Because she is responding to the wind, you don&amp;rsquo;t see any relationship between the movement of the rudder and the direction of the boat.
Another current example: social distancing and masking. We social distance and mask in response to the pandemic. But we don&amp;rsquo;t see the case count go down, we just see cases not change.</description></item><item><title>Choosing Prediction Over Explanation in Psychology</title><link>https://nguyenhphilip.github.io/notes/statistics/Choosing-Prediction-Over-Explanation-in-Psychology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Choosing-Prediction-Over-Explanation-in-Psychology/</guid><description>Abstract:
Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy.</description></item><item><title>Collinearity isn't une maladie that needs curing</title><link>https://nguyenhphilip.github.io/notes/statistics/Collinearity-isnt-une-maladie-that-needs-curing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Collinearity-isnt-une-maladie-that-needs-curing/</guid><description>Source .
Read this article when doing linear regression and are dealing with potential multicollinearity .
Every now and again, some worried student or collaborator asks me whether they’re “allowed” to fit a regression model in which some of the predictors are fairly strongly correlated with one another. Happily, most Swiss cantons have a laissez-faire policy with regard to fitting modelsmodèles with correlated predictors, so the answer to this question is “yes”.</description></item><item><title>confidence intervals</title><link>https://nguyenhphilip.github.io/notes/statistics/confidence-intervals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/confidence-intervals/</guid><description>the theoretical long-run proportion of confidence intervals that contain the true value of the unknown parameter being estimated
R Psychologist: 95% confidence is a confidence that in the long-run 95 % of the CIs will include the population mean. It is a confidence in the algorithm and not a statement about a single CI.
In other words, 90% of confidence intervals computed at the 90% confidence level contain the parameter, 95% of confidence intervals computed at the 95% confidence level contain the parameter, 99% of confidence intervals computed at the 99% confidence level contain the parameter, etc.</description></item><item><title>correlation</title><link>https://nguyenhphilip.github.io/notes/statistics/correlation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/correlation/</guid><description>Correlation is standardized covariance : $$corr(x,y) = \frac{Cov(x,y)}{sd(x)sd(y)}$$. It provides a value between -1 and 1 that denotes the steepness of a slope and its direction (positive or negative) between two data variables.</description></item><item><title>covariance</title><link>https://nguyenhphilip.github.io/notes/statistics/covariance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/covariance/</guid><description>#flashcards
Relation to variance variance alone cannot tell you about the relationship between two variables. Covariance, then, is a useful measure of how two variables relate, or covary, with each other.
Formally covariance is the expected product of two mean-centered variables:
$$Cov(X,Y) = E[(X-E(X))(Y-E(Y))]$$
It&amp;rsquo;s usually computed as:
$$Cov(X,Y) = \frac{1}{n-1}\sum_{i}{(x_i - \bar{x})}{(y_i - \bar{y})}$$
Some properties:
$$Cov(X,Y) = E(XY) - E(X)E(Y)$$
Relation to correlation Covariance gives direction of the relationship, but not strength.</description></item><item><title>effect size</title><link>https://nguyenhphilip.github.io/notes/statistics/effect-size/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/effect-size/</guid><description>definition a measure that determines the ==practical significance== of scientific results
An effect size is a standardized measurement that compares the size of some statistical effect to a reference quantity, such as the variability of the statistic.
why they&amp;rsquo;re useful They are a standardized metric (scale free) that enables comparison (e.g. meta analysis) and communication of results using effect sizes from prior studies is useful for planning new studies via statistical power by providing an indication of what sample size is necessary to observe a statistically significant result two families: D and r Effect sizes can be grouped in two families (Rosenthal, 1994): The d family (consisting of ==standardized mean differences==) and the r family (measures of ==strength of association==).</description></item><item><title>gaussian distribution</title><link>https://nguyenhphilip.github.io/notes/statistics/Gaussian-distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Gaussian-distribution/</guid><description>The PDF of the form: $$f(x) = \frac{1}{\sqrt{2{\pi}{\sigma^2}}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$
Expected mean: $$E[X] = \mu$$
Expected variance: $$E[(x-\mu)^2] = \sigma^2$$</description></item><item><title>gradient descent</title><link>https://nguyenhphilip.github.io/notes/statistics/Gradient-Descent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Gradient-Descent/</guid><description>definition A method of optimizing parameters in a model so that we minimize the loss function. The key idea is to update our parameter values using the slope of the loss function.
To do this we define a &amp;ldquo;step&amp;rdquo;, which is a predefined learning rate times the slope/derivative/gradient of our loss function at a particular parameter value, and add it to that particular parameter value. This is akin to moving in the direction of the gradient until we reach the bottom of the loss function.</description></item><item><title>KL Divergence</title><link>https://nguyenhphilip.github.io/notes/statistics/KL-Divergence/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/KL-Divergence/</guid><description>Intuition A tool to measure how different two probability distributions are.
The motivation is that we often want to model observed data using a simpler approximated distribution (q) if we think that the data are generated by a complex distribution (p). KL Divergence measures how much information we lose under the approximated distribution. It is the expected information loss.
Since we&amp;rsquo;re concerned with information loss, we first need a measure of information.</description></item><item><title>linear regression</title><link>https://nguyenhphilip.github.io/notes/statistics/linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/linear-regression/</guid><description>#flashcards
Multiple Linear Regression The defining question of multiple linear regression is: What is the value of knowing each predictor, once we already know the other predictors? Implicit in this question are: (1) a focus on the value of the predictors for description of the sample, instead of forecasting a future sample; and (2) the assumption that the value of each predictor does not depend upon the values of the other predictors.</description></item><item><title>Monte Carlo methods</title><link>https://nguyenhphilip.github.io/notes/statistics/Monte-Carlo-methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Monte-Carlo-methods/</guid><description>via Probabilistic ML:
Algorithms that compute expectations by repeated random sampling, using samples $x_i$ ~ $p(x)$
Examples: $$\int f(x)p(x)dx \approx \frac{1}{S} \sum_{i=1}^S f(x_i)$$
$$\int p(x,y)dx \approx \sum_{i} p(y | x_i);$$
Caculating the ratio of a triangle&amp;rsquo;s area within a square. Sample a point. If it&amp;rsquo;s in the triangle, record that. Otherwise, it was part of the square.
Why is this useful? integrals are difficult
Monte Carlo works on every Integral</description></item><item><title>multicollinearity</title><link>https://nguyenhphilip.github.io/notes/statistics/multicollinearity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/multicollinearity/</guid><description>Multicollinearity refers to two or more predictor variables that are correlated with each other.
What&amp;rsquo;s the problem with correlated predictors in linear regression ? It can be misleading.
The consequence of multicollinearity is that ==the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome.== This frustrating phenomenon arises from the details of how linear regression works.</description></item><item><title>Null Hypothesis Significance Testing</title><link>https://nguyenhphilip.github.io/notes/statistics/Null-Hypothesis-Significance-Testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Null-Hypothesis-Significance-Testing/</guid><description>Given that the null hypothesis is true, how unlikely is the data? $$p(Data|H_0 = \text{True})$$
If the likelihood of the data is unlikely (judged by a decision threshold: the p-value ), given that the null hypothesis is true, then we can reject the null hypothesis in favor of the alternative hypothesis The steps of NHST Formulate a hypothesis that embodies our prediction (without seeing the data) Specify the null and alternative hypotheses Collect data relevant to the hypothesis Fit a model to the data that represents the alternative hypothesis and compute a test statistic Compute the probability of observing the value of that test statistic assuming null hypothesis is true Assess the &amp;ldquo;statistical significance&amp;rdquo; of that result Common tests For comparing group mean differences: t-statistic and t-test</description></item><item><title>Ulysses' Compass - Ch 7 Statistical Rethinking</title><link>https://nguyenhphilip.github.io/notes/statistics/Chapter-7-Statistical-Rethinking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Chapter-7-Statistical-Rethinking/</guid><description>Ulysses' Compass A chapter about overfitting, underfitting, prediction models vs. causal mechanism models.
The problem with Parameters Adding more parameters to a model will usually improve prediction, despite the lacking causal associations. That&amp;rsquo;s because the model can retrodict the data to fit the model.
variance explained (by R-squared ):
$R^2 = \frac{var(outcome)-var(residuals)}{var(outcome)} = 1 - \frac{var(residuals)}{var(outcome)}$ $R^2$ increases even if we add random numbers as predictors that have no association to the outcome Entropy and Accuracy In navigating overfitting and underfitting, we must choose some measure of model performance.</description></item></channel></rss>