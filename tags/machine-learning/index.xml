<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine-learning on</title><link>https://nguyenhphilip.github.io/tags/machine-learning/</link><description>Recent content in machine-learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://nguyenhphilip.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>bias-variance tradeoff</title><link>https://nguyenhphilip.github.io/notes/statistics/bias-variance-tradeoff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/bias-variance-tradeoff/</guid><description>variance and bias refer to under and overfitting on the data.
Wikipedia on the bias-variance tradeoff: bias is error from erroneous assumptions in the ==model/learning algorithm== [e.g. using linear models when the data is non-linear -&amp;gt; high bias, more generalizable; not using enough predictors or predictors that may be important to the outcome -&amp;gt; high bias]. High bias causes the model to miss the relevant relations between features and target outputs, which leads to ==underfitting==.</description></item><item><title>Choosing Prediction Over Explanation in Psychology</title><link>https://nguyenhphilip.github.io/notes/statistics/Choosing-Prediction-Over-Explanation-in-Psychology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Choosing-Prediction-Over-Explanation-in-Psychology/</guid><description>Abstract:
Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychologyâ€™s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy.</description></item><item><title>gradient descent</title><link>https://nguyenhphilip.github.io/notes/statistics/Gradient-Descent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Gradient-Descent/</guid><description>definition A method of optimizing parameters in a model so that we minimize the loss function. The key idea is to update our parameter values using the slope of the loss function.
To do this we define a &amp;ldquo;step&amp;rdquo;, which is a predefined learning rate times the slope/derivative/gradient of our loss function at a particular parameter value, and add it to that particular parameter value. This is akin to moving in the direction of the gradient until we reach the bottom of the loss function.</description></item></channel></rss>