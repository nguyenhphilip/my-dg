<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bayesian on</title><link>https://nguyenhphilip.github.io/tags/bayesian/</link><description>Recent content in bayesian on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://nguyenhphilip.github.io/tags/bayesian/index.xml" rel="self" type="application/rss+xml"/><item><title>Bayes Factor</title><link>https://nguyenhphilip.github.io/notes/statistics/Bayes-factor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Bayes-factor/</guid><description>A video introduction</description></item><item><title>binomial distribution</title><link>https://nguyenhphilip.github.io/notes/statistics/binomial-distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/binomial-distribution/</guid><description>The probability of $k$ successful outcomes over $n$ trials is given by : $$P_k = {n \choose k}{p^k}{(1-p)^{n-k}}$$
The binomial distribution is the most consistent distribution - it maximizes entropy - given these constraints:
only two unordered outcomes are possible expected number of each type is constant expected value: np</description></item><item><title>Choosing Prediction Over Explanation in Psychology</title><link>https://nguyenhphilip.github.io/notes/statistics/Choosing-Prediction-Over-Explanation-in-Psychology/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Choosing-Prediction-Over-Explanation-in-Psychology/</guid><description>Abstract:
Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy.</description></item><item><title>Collinearity isn't une maladie that needs curing</title><link>https://nguyenhphilip.github.io/notes/statistics/Collinearity-isnt-une-maladie-that-needs-curing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Collinearity-isnt-une-maladie-that-needs-curing/</guid><description>Source .
Read this article when doing linear regression and are dealing with potential multicollinearity .
Every now and again, some worried student or collaborator asks me whether they’re “allowed” to fit a regression model in which some of the predictors are fairly strongly correlated with one another. Happily, most Swiss cantons have a laissez-faire policy with regard to fitting modelsmodèles with correlated predictors, so the answer to this question is “yes”.</description></item><item><title>KL Divergence</title><link>https://nguyenhphilip.github.io/notes/statistics/KL-Divergence/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/KL-Divergence/</guid><description>Intuition A tool to measure how different two probability distributions are.
The motivation is that we often want to model observed data using a simpler approximated distribution (q) if we think that the data are generated by a complex distribution (p). KL Divergence measures how much information we lose under the approximated distribution. It is the expected information loss.
Since we&amp;rsquo;re concerned with information loss, we first need a measure of information.</description></item><item><title>Monte Carlo methods</title><link>https://nguyenhphilip.github.io/notes/statistics/Monte-Carlo-methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Monte-Carlo-methods/</guid><description>via Probabilistic ML:
Algorithms that compute expectations by repeated random sampling, using samples $x_i$ ~ $p(x)$
Examples: $$\int f(x)p(x)dx \approx \frac{1}{S} \sum_{i=1}^S f(x_i)$$
$$\int p(x,y)dx \approx \sum_{i} p(y | x_i);$$
Caculating the ratio of a triangle&amp;rsquo;s area within a square. Sample a point. If it&amp;rsquo;s in the triangle, record that. Otherwise, it was part of the square.
Why is this useful? integrals are difficult
Monte Carlo works on every Integral</description></item><item><title>Ulysses' Compass - Ch 7 Statistical Rethinking</title><link>https://nguyenhphilip.github.io/notes/statistics/Chapter-7-Statistical-Rethinking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nguyenhphilip.github.io/notes/statistics/Chapter-7-Statistical-Rethinking/</guid><description>Ulysses' Compass A chapter about overfitting, underfitting, prediction models vs. causal mechanism models.
The problem with Parameters Adding more parameters to a model will usually improve prediction, despite the lacking causal associations. That&amp;rsquo;s because the model can retrodict the data to fit the model.
variance explained (by R-squared ):
$R^2 = \frac{var(outcome)-var(residuals)}{var(outcome)} = 1 - \frac{var(residuals)}{var(outcome)}$ $R^2$ increases even if we add random numbers as predictors that have no association to the outcome Entropy and Accuracy In navigating overfitting and underfitting, we must choose some measure of model performance.</description></item></channel></rss>