---
title: KL Divergence
aliases: [Kullback-Leibler divergence]
tags: [Bayesian, statistics]
creation date: 2021-07-28 12:24
---

# Intuition

A tool to measure how different two probability distributions are.

The motivation is that we often want to model observed data using a simpler approximated distribution (q) if we think that the data are generated by a complex distribution (p). KL Divergence measures how much information we lose under the approximated distribution. It is the expected information loss. 

Since we're concerned with information loss, we first need a measure of information. That's where [information theory](notes/statistics/information-theory.md) comes in, specifically the concept of [entropy](notes/statistics/entropy.md). The [entropy](notes/statistics/entropy.md) associated with a random variable $X$ is given as $$H(X) = E[-log(P(X))] = \sum_i^N p(x_i)log(p(x_i))$$ which is the average or expected uncertainty surprise of $X$.

KL divergence is a slightly modified version of [entropy](notes/statistics/entropy.md) where we add in $q$ and look at the average log difference between $log(q)$ and $log(p)$: $$D_{KL}(p||q) = \sum_i^N p(x_i)[log(p(x_i)) - log(q(x_i))]$$. In expectation form $$D_{KL}(p\\q) = E[log(\frac{p(x)}{q(x)}))] = E[log(p) - log(q)]$$.

Note that KL divergence is not a measure of distance. It is not symmetric: $$D_{KL}(p||q) \neq D_{KL}(q||p)$$ 

# KL divergence as a loss function

The idea is that we can use KL divergence as a loss function in approximating models by minimizing information loss. variational autoencoders do just this with neural networks: they can learn complex approximating distributions for a given set of data. 

One advantage of this is that it is an optimization method – which we're really good at, ala Dave – that can be used to do inference on difficult integrals in [Bayesian Statistics](notes/statistics/Bayesian-Statistics.md). [Monte Carlo methods](notes/statistics/Monte-Carlo-methods.md) would have trouble here because the integral may be too computationally expensive.

# References
- [Countbayesie for intuition and simple example](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
