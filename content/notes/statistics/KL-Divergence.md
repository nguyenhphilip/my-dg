---
title: KL Divergence
aliases: [Kullback-Leibler divergence]
tags: [Bayesian, statistics]
creation date: 2021-07-28 12:24
---

# Intuition

A tool to measure how different two probability distributions are.

The motivation is that we often want to model observed data using a simpler approximated distribution (Q) if we think that the data are generated by a complex, intractable distribution (P). KL Divergence measures how much information we expect to lose (in units of bits) under the approximated distribution (Q). $$D_{KL}(P||Q) = E_p[\log\frac{p(x)}{q(x)}]$$

Since we're concerned with information loss, we first need a measure of information. That's where [information theory](notes/statistics/information-theory.md) comes in, specifically the concept of [entropy](notes/statistics/entropy.md). The entropy associated with a random variable $X$ is given as $$H(X) = E[-log(P)] = -\sum_i^N p(x_i)log(p(x_i))$$ which can be viewed as the average or expected surprisal of a random variable $X$.

KL divergence modifies entropy in that we add look at the expected log difference between $q$ and $p$: $$D_{KL}(p||q) = \sum_i^N p(x_i)[log(p(x_i)) - log(q(x_i))] = -\sum_i^N p(x_i) [log(q(x_i)) - log(p(x_i))]$$. 

Written another way, the KL divergence is the cross entropy of the true distribution and the approximate distribution minus the entropy of the true distribution: $$D_{KL}(P||Q) = H(P,Q) - H(P) = -[\sum_i^N p(x_i)\log{q(x_i)} -\sum_i^N p(x_i)\log{p(x_i)}] $$

Note that KL divergence is not a measure of distance. It is not symmetric: $$D_{KL}(p||q) \neq D_{KL}(q||p)$$ 

# KL divergence as a loss function

The idea is that we can use KL divergence as a loss function in approximating models by minimizing information loss. Variational [Autoencoders](notes/statistics/Autoencoders.md) do this with neural networks: they can learn complex approximating distributions for a given set of data. 

One advantage of this is that it is an optimization method – which we're really good at, ala Dave – that can be used to do inference on difficult integrals in [Bayesian Statistics](notes/statistics/Bayesian-Statistics.md). [Monte Carlo methods](notes/statistics/Monte-Carlo-methods.md) would have trouble here because the integral may be too computationally expensive.

# References
- [Countbayesie for intuition and simple example](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
