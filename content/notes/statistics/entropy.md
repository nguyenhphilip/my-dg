---
title: entropy
creation date: 2021-07-28 12:35
---

The average amount of information, surprise, or uncertainty associated with given a random variable's possible outcomes.

$$H(X) = E[-log(P(X))] = -\sum_i^N p(x_i)log(p(x_i)) $$

If $log$ base $2$ is used, then the units are in bits and represent the minimum bits of information needed to encode the information associated with $X$. 

